import { Fragment } from 'react';
import PostDetails from '../../components/PostDetails';

export const config = { unstable_runtimeJS: false };

export const postDetails = (preview = false, postPath = '') => (
    <PostDetails
        preview={preview}
        path={postPath}
        title="Using Information Theory to Find Critical Scout Timings in StarCraft 2"
        postedAt="WIP"
    />
);

<Fragment>{postDetails()}</Fragment>

- Information Theory and Shannon Entropy
- Apply Shannon Entropy to builds by calculating frequency of appearance
 - Naive calculation, room for improvement by calculating frequency based on time
 - Another possibility is calculating frequency based on tech requirements being met
- Use frequencies to generate probabilities
- Input build from match and calculate entropy for each building
- Generate chart of entropy of each building
- High entropy = more surprise, so maybe you should find out what's happening!
- On initial observation, it seems relatively accurate!
- Tried units as well but doesn't seem as useful
- Issues/Problems
 - Frequency does not always mean surprise. Reapers for example have low frequency
 - Frequencies don't take time into account

Depending on your background, your first reaction to that title probably ranged from "I know some of those words"
to "this makes no sense". Either way, in a few short paragraphs it'll make perfect sense so don't sweat it!

First I'll explain the small piece of Information Theory we're going to utilize
(Shannon Entropy) and the idea I had to apply it to StarCraft 2 (SC2).

Then I'll walk through actually applying the idea in practice and what the preliminary
results looked like.

Lastly, I'll go through some possible improvements of my method and other applications of the idea.

## Shannon Entropy

Information Theory is all about quantifying information and it has numerous applications such
as error correction, compression and cryptography.

One of *the* core ideas in Information Theory is the concept of entropy (A.k.a Shannon Entropy),
which is equivalent to uncertainty, information or surprise.

The idea is that the more uncertain an event is, the more information it has. Let's compare
a fair coin with an unfair one to see why this makes sense.

    # fair coin
    P(Tails) = 0.5, P(Heads) = 0.5

    # unfair coin
    P(Tails) = 0.9, P(Heads) = 0.1

If we flip the unfair coin, we expect that it will land on Tails because that's the most likely
outcome. However, if we flip the fair coin we're not sure whether it will land on Heads or Tails
because they're equally likely.

In other words, if the unfair coin lands on Tails it's *not very surprising* whereas the fair
coin is somewhat surprising no matter what it lands on!

This is reflected in the entropy of these distributions. The fair coin has a higher entropy than
the unfair coin. Speaking of which, the equation for entropy that we will be using is as follows:

    # log base 2
    H(X) = -log2(P(x))

Where `P(x)` is the probability of `x` ocurring. I'm not going to explain this equation, but the TL;DR
is that high probability = low entropy and vice versa. This is also the equation for a single
event, not a probability distribution like the coins. If you'd like to read more about Shannon Entropy
you can find information on [it's wikipedia page](https://en.wikipedia.org/wiki/Entropy_(information_theory)).

Relating this back to information, we can view surprise as information because if an unsurprising
event occurs we haven't gained much information since we already knew it was likely to occur, and vice
versa for surprising events.

A common application of entropy is measuring the entropy (I.e. information content!) of text.
We can measure the frequency of different words or letters, create probabilities based on those frequencies
and then calculate the entropy of arbitrary text.

An example of this is using the alphabet as our source of frequency/probability and measuring
the entropy of some text. Another example is using a whole pile of books as the source
of frequency/probability for words, then measuring the entropy of a book.

## Applying Entropy to StarCraft 2

So how does this relate to scouting? Well, scouting is about gathering information.
Maybe if we can quantify the amount of information in the game at each point in time then we
can figure out when there's a lot of new information, and that seems like good time to scout.

Figuring out what build your opponent is doing is generally a pretty important piece of
information, so we'll start by trying to measure the entropy of that (I'll talk about units later).

If we think about a build, it's just a list of different buildings which is basically the same
thing as a list of words. We can measure the frequency of each building over many games,
calculate probabilities based on those frequencies and then use those to calculate the
entropy of each building in a build.

Then we can plot the entropy of each building against time and judge if it's a good time to scout
based on the relative entropy of each building. High entropy relative to other buildings
indicates a surprising event occured, so we might want to scout what happened.

## Calculating Entropy from IEM Katowice Games

First, I made a list of buildings to ignore when counting frequencies since I thought they would
negatively affect the interpretability of entropy. I eliminated supply buildings (Pylon, Supply Depot),
gas collectors and some support buildings like Missile Turrets and Cannons.

This is the full list:

    IGNORE_OBJECTS = [
        'Pylon',
        'Overlord',
        'SupplyDepot',
        'MissileTurret',
        'SpineCrawler',
        'SporeCrawler',
        'PhotonCannon',
        'Interceptor',
        'MULE',
        'AutoTurret',
        'Larva',
        'Egg',
        'LocustMP',
        'LocustMPPrecursor',
        'LocustMPFlying',
        'ShieldBattery',
        'Bunker',
        'CreepTumor',
        'CreepTumorQueen',
        'CreepTumorBurrowed',
        'Broodling',
        'BroodlingEscort',

        # remove gas buildings and workers to reduce noise
        'Extractor',
        'Assimilator',
        'Refinery',
        'Probe',
        'Drone',
        'SCV',
    ]

And here's a quick breakdown of my reasoning for eliminating each of these types:

### Supply Buildings

Supply buildings are so abundant that including them would heavily affect the frequency of all
buildings. We also don't generally care that much about supply buildings, so it seems like a lot
of noise for little information.

### Gas Collectors

At first I included gas collectors in the analysis, but I found that they were too common and
although gas timings can be important, any given gas collector is **not** important so I scrapped them.

### Support Buildings

Because this analysis is focused on scouting, I thought that removing these buildings would reduce
the amount of noise since we generally don't care about these types of buildings when we're scouting.
There is the case where someone is turtling and making a lot of these buildings, but I'm not
sure if the benefit of accounting for this case outweighs the disadvantage of creating more noise
for other scenarios.

Next, I calculated building frequencies on a per matchup basis to try and increase accuracy and
then converted those to probabilities.
