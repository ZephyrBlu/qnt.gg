import { Fragment } from 'react';
import PostDetails from '../../components/PostDetails';

export const config = { unstable_runtimeJS: false };

export const postDetails = (preview = false, postPath = '') => (
    <PostDetails
        preview={preview}
        path={postPath}
        title="Using Information Theory to Find Critical Scout Timings in StarCraft 2"
        postedAt="WIP"
    />
);

<Fragment>{postDetails()}</Fragment>

- Information Theory and Shannon Entropy
- Apply Shannon Entropy to builds by calculating frequency of appearance
 - Naive calculation, room for improvement by calculating frequency based on time
 - Another possibility is calculating frequency based on tech requirements being met
- Use frequencies to generate probabilities
- Input build from match and calculate entropy for each building
- Generate chart of entropy of each building
- High entropy = more surprise, so maybe you should find out what's happening!
- On initial observation, it seems relatively accurate!
- Tried units as well but doesn't seem as useful

Depending on your background, your first reaction to that title probably ranged from "I know some of those words"
to "this makes no sense". Either way, in a few short paragraphs it'll make perfect sense so don't sweat it!

First I'll explain the small piece of Information Theory we're going to utilize
(Shannon Entropy) and the idea I had to apply it to StarCraft 2 (SC2).

Then I'll walk through actually applying the idea in practice and what the preliminary
results looked like.

Lastly, I'll go through some possible improvements of my method and other applications of the idea.

## Shannon Entropy

Information Theory is all about quantifying information and it has numerous applications such
as error correction, compression and cryptography.

One of *the* core ideas in Information Theory is the concept of entropy (A.k.a Shannon Entropy),
which is equivalent to uncertainty, information or surprise.

The idea is that the more uncertain an event is, the more information it has. Let's compare
a fair coin with an unfair one to see why this makes sense.

    # fair coin
    P(Tails) = 0.5, P(Heads) = 0.5

    # unfair coin
    P(Tails) = 0.9, P(Heads) = 0.1

If we flip the unfair coin, we expect that it will land on Tails because that's the most likely
outcome. However, if we flip the fair coin we're not sure whether it will land on Heads or Tails
because they're equally likely.

In other words, if the unfair coin lands on Tails it's *not very surprising* whereas the fair
coin is somewhat surprising no matter what it lands on!

This is reflected in the entropy of these distributions. The fair coin has a higher entropy than
the unfair coin. Speaking of which, the equation for entropy that we will be using is as follows:

    # log base 2
    H(X) = -log2(P(x))

Where `P(x)` is the probability of `x` ocurring. I'm not going to explain this equation, but the TL;DR
is that high probability = low entropy and vice versa. This is also the equation for a single
event, not a probability distribution like the coins. If you'd like to read more about Shannon Entropy
you can find information on [it's wikipedia page](https://en.wikipedia.org/wiki/Entropy_(information_theory)).

Relating this back to information, we can view surprise as information because if an unsurprising
event occurs we haven't gained much information since we already knew it was likely to occur, and vice
versa for surprising events.

A common application of entropy is measuring the entropy (I.e. information content!) of text.
We can measure the frequency of different words or letters, create probabilities based on those frequencies
and then calculate the entropy of arbitrary text.

An example of this is using the alphabet as our source of frequency/probability and measuring
the entropy of some text. Another example is using a whole pile of books as the source
of frequency/probability for words, then measuring the entropy of a book.

## Applying Entropy to StarCraft 2

So, how does this relate to scouting?! Well, scouting is about gathering information.
Maybe if we quantify the amount of information in the game at each point in time then we could
figure out when there's a lot of new information, and that seems like good time to scout.

Figuring out what build your opponent is doing is generally a pretty important piece of
information, so we'll start trying to measure the entropy of that (I'll talk about units later).

If we think about a build, it's just a list of different buildings which is basically the same
thing as a list of words. So we could just measure the frequency of each building, calculate probabilities
based on those frequencies and then use those to calculate the entropy of each building.

## Measuring the Information Content of StarCraft 2 Builds


